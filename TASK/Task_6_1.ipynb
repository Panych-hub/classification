{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
    "- `news_train.txt` тренировочное множество\n",
    "- `news_test.txt` тренировочное множество\n",
    "\n",
    "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
    "\n",
    "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
    "\n",
    ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "    - pymorphy2\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "    \n",
    "    \n",
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
    "\n",
    "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия\n",
    "    \n",
    "\n",
    "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.pitt.edu/~naraehan/presentation/word2vec-try.html\n",
    "https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 3000)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = list(open('./news_train.txt', 'r', encoding='utf-8'))\n",
    "lines_test = list(open('./news_test.txt', 'r', encoding='utf-8'))\n",
    "len(lines), len(lines_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_train = [line.split('\\t')[0] for line in lines]\n",
    "head_train = [line.split('\\t')[1] for line in lines]\n",
    "text_train = [line.split('\\t')[2] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_test = [line.split('\\t')[0] for line in lines_test]\n",
    "head_test = [line.split('\\t')[1] for line in lines_test]\n",
    "text_test = [line.split('\\t')[2] for line in lines_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'business': 554,\n",
       "          'economics': 2080,\n",
       "          'style': 284,\n",
       "          'science': 2156,\n",
       "          'media': 2111,\n",
       "          'life': 2033,\n",
       "          'forces': 1225,\n",
       "          'culture': 2053,\n",
       "          'sport': 2215,\n",
       "          'travel': 289}),\n",
       " Counter({'culture': 426,\n",
       "          'media': 403,\n",
       "          'business': 90,\n",
       "          'life': 415,\n",
       "          'science': 466,\n",
       "          'forces': 245,\n",
       "          'sport': 423,\n",
       "          'economics': 426,\n",
       "          'travel': 54,\n",
       "          'style': 52}))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([line.split('\\t')[0] for line in lines[:15000]]),\\\n",
    "Counter([line.split('\\t')[0] for line in lines_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Обработать данные, получив для каждого текста набор токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# nltk.download('punkt')\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import RussianStemmer \n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "russian_stopwords += ['что', 'это', 'так', 'вот', 'быть', '—', '–', 'к',  '...']\n",
    "rs = RussianStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    symbols = \"«»0123456789!\\\"#$%&()*+./:;<=>?@[\\]^_`{|}~=\\n,-\"\n",
    "    for j in symbols:\n",
    "        text = text.replace(j, ' ')\n",
    "\n",
    "    tokens = [token for token in text.lower().split() if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "\n",
    "    tokens = [rs.stem(k) for k in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_tokens = []\n",
    "for text in text_train:\n",
    "    text_train_sent = sent_tokenize(text)\n",
    "    text_tokens = []\n",
    "    for sent in text_train_sent:\n",
    "        text_tokens += preprocess_text(sent) \n",
    "    text_train_tokens += [text_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_proc = [' '.join(i) for i in text_train_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test_tokens = []\n",
    "for text in text_test:\n",
    "    text_test_sent = sent_tokenize(text)\n",
    "    text_tokens = []\n",
    "    for sent in text_test_sent:\n",
    "        text_tokens += preprocess_text(sent) \n",
    "    text_test_tokens += [text_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test_proc = [' '.join(i) for i in text_test_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать gensim . Продемонстрировать семантические ассоциации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=text_train_tokens, \n",
    "                 window=5, \n",
    "                 min_count=5, \n",
    "                 workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Word2Vec(sentences=text_train_tokens, \n",
    "#                  min_count=5,\n",
    "#                  window=5,\n",
    "#                  size=300,\n",
    "#                  sample=6e-5, \n",
    "#                  alpha=0.03, \n",
    "#                  min_alpha=0.0007, \n",
    "#                  negative=20,\n",
    "#                  workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=25344, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "# print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('август', 0.651009738445282),\n",
       " ('июн', 0.6471593379974365),\n",
       " ('ноябр', 0.6432275772094727),\n",
       " ('ма', 0.6417557001113892),\n",
       " ('март', 0.6366109251976013),\n",
       " ('феврал', 0.6365525722503662),\n",
       " ('октябр', 0.635940432548523),\n",
       " ('сентябр', 0.6286870837211609),\n",
       " ('январ', 0.6254435777664185),\n",
       " ('апрел', 0.623516321182251),\n",
       " ('декабр', 0.6114797592163086),\n",
       " ('поздн', 0.5123351812362671)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['июл'], negative=['shell'], topn=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('присоединен', 0.7136656045913696),\n",
       " ('транспорт', 0.6735888719558716),\n",
       " ('таможен', 0.6712261438369751),\n",
       " ('киев', 0.6687359809875488),\n",
       " ('республик', 0.6652190685272217),\n",
       " ('введен', 0.6639152765274048),\n",
       " ('украин', 0.6625280380249023),\n",
       " ('туризм', 0.6515752077102661),\n",
       " ('турц', 0.6496397256851196),\n",
       " ('регион', 0.6457705497741699)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['торговл', 'крым'], negative=['shell'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from matplotlib import pyplot\n",
    "\n",
    "# fig = plt.figure()\n",
    "# fig.set_size_inches(18.5, 10.5)\n",
    "# X = model.wv[model.wv.vocab]\n",
    "# pca = PCA(n_components=2)\n",
    "# result = pca.fit_transform(X)\n",
    "# pyplot.scatter(result[:, 0], result[:, 1])\n",
    "\n",
    "# for i, word in enumerate(text_train_tokens[2]):\n",
    "#     pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(type_train)\n",
    "Test_Y = Encoder.fit_transform(type_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=15000)\n",
    "Tfidf_vect.fit(text_train_proc)\n",
    "Train_X_Tfidf = Tfidf_vect.transform(text_train_proc)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(text_test_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "наивный байесовский классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  83.6\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf, Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \", accuracy_score(predictions_NB, Test_Y) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8742666666666666\n",
      "LogisticRegression(C=8.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='multinomial', n_jobs=4, penalty='l2',\n",
      "                   random_state=17, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyper = {'C' : np.linspace(4, 8, 5),\n",
    "         'solver': ['newton-cg', 'sag', 'saga', 'lbfgs']}\n",
    "\n",
    "gd=GridSearchCV(estimator=LogisticRegression(multi_class='multinomial', random_state=17, n_jobs=4), param_grid=hyper)\n",
    "\n",
    "gd.fit(Train_X_Tfidf, Train_Y)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Accuracy Score ->  89.0\n"
     ]
    }
   ],
   "source": [
    "logit = gd.best_estimator_\n",
    "logit.fit(Train_X_Tfidf, Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_logit = logit.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Logistic regression Accuracy Score -> \", accuracy_score(predictions_logit, Test_Y) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  88.56666666666668\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf, Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, Test_Y) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
